# -*- coding: utf-8 -*-
"""Cloud_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JsSWjcAcFLDbR0vwlExt6Dz8hNZVjd5N
"""

# Units are measured in meters

Highway_Total_distance = 11000

Coverage = 100 

AP0 = 50

AP2 = 2050

AP3 = 4050

AP4 = 6050

AP5 = 8050

AP6 = 10050

"""## Strategy 1
This is base line strategy can be used when vehicular cloud was implemented on highway but this base line strategy drops heavy jobs will improve this strategy in next steps of this project 
"""

import pandas as pd
car_job_information_df = pd.DataFrame(columns=['Vehicle_ID', 'Vehicle_speed','Arrival time','Residency_time_seconds', 'Job_ID_assigned', 'Job_total_completion_time'])

#Maximum highway speed 60 kilometers per hour or 60000 meters per hour
#Highway patrol police make sure that cars entered are not jammed in traffic on highway

import random
import numpy as np
import string
import time


# defining function for random
# string id with parameter
def ran_gen(size, chars=string.ascii_uppercase + string.digits):
    return ''.join(random.choice(chars) for x in range(size))

car_job_information_dict = {}

for i in range(100): 
  # function call for random string
  # generation with size 8 and string
  Vehicle_number = ran_gen(8, "AEIOSUMA23")
  car_job_information_dict['Vehicle_ID'] = Vehicle_number

  np.random.seed(random.randint(0,100000))
  Vehicle_arrival_time = random.randint(0,3600) # in seconds
  car_job_information_dict['Arrival time'] = Vehicle_arrival_time


  import time

  # np.random.seed(int(time.time()))
  np.random.seed(random.randint(0,100000))
  In_speed_kilometers_per_hour = abs(np.random.normal(size = 4, loc = 40, scale = 10)[0]) # geneating speed mean = 30 , standard_deviation = 20

  In_speed_meters_per_second = In_speed_kilometers_per_hour * 1000/3600
  car_job_information_dict['Vehicle_speed'] = In_speed_meters_per_second

  Residency_time_seconds = Highway_Total_distance/In_speed_meters_per_second
  car_job_information_dict['Residency_time_seconds'] = Residency_time_seconds

  Job_id_number = ran_gen(3, "AEIOSUMA23")
  Job_size = 10 #mb

  np.random.seed(random.randint(0,100000))
  Job_Processing = abs(np.random.normal(size = 4, loc = 700, scale = 40)[0])  # geneating job processing time mean = 400 , standard_deviation = 40

  Job_download_time = 1 # seconds
  Job_upload_time = 1 # seconds
  Job_total_completion_time = Job_download_time + Job_Processing + Job_upload_time

  if(Residency_time_seconds > Job_total_completion_time):
    job_assigned = Job_id_number
  else:
    job_assigned = 0

  print("Vehice number is", Vehicle_number)
  print("Speed of Vehicle entered is", In_speed_kilometers_per_hour, "Km/Hour")
  print("Residency time of vehicle",Residency_time_seconds, "Seconds")


  if(job_assigned == 0):
    print("No Job assigned")
    car_job_information_dict['Job_ID_assigned'] = 0
    car_job_information_dict['Job_total_completion_time'] = 0
  else:
    print("ID of the job assigned is",Job_id_number)
    print("Size of the job is", Job_size)
    print("Total Job Processing time", Job_total_completion_time)
    car_job_information_dict['Job_ID_assigned'] = job_assigned
    car_job_information_dict['Job_total_completion_time'] = Job_total_completion_time

  # print(car_job_information_dict)
  car_job_information_df = car_job_information_df.append(car_job_information_dict, ignore_index=True)
  car_job_information_dict.clear()


# car_job_information_df['Residency_time_seconds']=car_job_information_df['Residency_time_seconds'].astype(int)
# car_job_information_df['Job_total_completion_time']=car_job_information_df['Job_total_completion_time'].astype(int)

# print(car_job_information_df)
csv_data = car_job_information_df.to_csv("Car_and_Job_information_strategy_1.csv")

"""Graph between Residency time and total job completion time"""

car_job_information_df.plot(x='Residency_time_seconds', y='Job_total_completion_time', style='o')

"""## Strategy 2
Instead of dropping jobs push jobs into stack which has high job completion time.
When ever vehicle comes calculate residency time of the vechicle and check whether is there any jobs that has total job completion time less than residency time of vehicle and if yes then assign that job on that stack to that vehicle
If no such job is found then assign new job to that vehicle if total job completion time is lesser than residency time of the vehicle
"""

import csv
with open('stack.csv', 'w') as creating_new_csv_file: 
  writer = csv.writer(creating_new_csv_file)
  writer.writerow(['Job_ID_assigned', 'Job_total_completion_time','status']) 
print("Empty File Created Successfully")

import pandas as pd
job_stack_df = pd.DataFrame(columns=['Job_ID_assigned', 'Job_total_completion_time','status'])
car_job_information_df = pd.DataFrame(columns=['Vehicle_ID', 'Vehicle_speed','Arrival time','Residency_time_seconds', 'Job_ID_assigned', 'Job_total_completion_time'])

job_stack_df.drop(job_stack_df.index, inplace=True)
car_job_information_df.drop(car_job_information_df.index, inplace=True)



#Maximum highway speed 60 kilometers per hour or 60000 meters per hour
#Highway patrol police make sure that cars entered are not jammed in traffic on highway

import random
import numpy as np
import string
import time



# defining function for random
# string id with parameter
def ran_gen(size, chars=string.ascii_uppercase + string.digits):
    return ''.join(random.choice(chars) for x in range(size))

car_job_information_dict = {}

for i in range(100): 
  # function call for random string
  # generation with size 8 and string
  Vehicle_number = ran_gen(8, "AEIOSUMA23")
  car_job_information_dict['Vehicle_ID'] = Vehicle_number

  np.random.seed(random.randint(0,100000))
  Vehicle_arrival_time = random.randint(0,3600) # in seconds
  car_job_information_dict['Arrival time'] = Vehicle_arrival_time


  import time

  # np.random.seed(int(time.time()))
  np.random.seed(random.randint(0,100000))
  In_speed_kilometers_per_hour = abs(np.random.normal(size = 4, loc = 40, scale = 10)[0]) # geneating speed mean = 30 , standard_deviation = 20

  In_speed_meters_per_second = In_speed_kilometers_per_hour * 1000/3600
  car_job_information_dict['Vehicle_speed'] = In_speed_meters_per_second

  Residency_time_seconds = Highway_Total_distance/In_speed_meters_per_second
  car_job_information_dict['Residency_time_seconds'] = Residency_time_seconds

  found = 0
  with open('stack.csv') as file_obj:
      
    # Create reader object by passing the file 
    # object to reader method
    reader_obj = csv.reader(file_obj)
      
    # Iterate over each row in the csv 
    # file using reader object
    for row in reader_obj:
        print(row)
        if(row[2]=="not_assigned" ):
          print(row[1])
          if(Residency_time_seconds > float(row[1]) ):
            found = 1
            car_job_information_dict['Job_ID_assigned'] = row[0]
            car_job_information_dict['Job_total_completion_time'] = row[1]
  
  if(found == 0):
    Job_id_number = ran_gen(3, "AEIOSUMA23")
    Job_size = 10 #mb

    np.random.seed(random.randint(0,100000))
    Job_Processing = abs(np.random.normal(size = 4, loc = 700, scale = 40)[0])  # geneating job processing time mean = 400 , standard_deviation = 40

    Job_download_time = 1 # seconds
    Job_upload_time = 1 # seconds
    Job_total_completion_time = Job_download_time + Job_Processing + Job_upload_time

    if(Residency_time_seconds > Job_total_completion_time):
      job_assigned = Job_id_number
    else:
      job_assigned = 0
      fields = []
      fields.append(job_assigned)
      fields.append(Job_total_completion_time)
      fields.append("not_assigned") 
      with open(r'stack.csv', 'a') as f:
        writer = csv.writer(f)
        writer.writerow(fields)
        fields.clear()
      # stack_df_refine = pd.read_csv('stack.csv')
      # df_new = stack_df_refine[stack_df_refine["Job_ID_assigned"].str.contains(job_assigned) == False]
      # df_new.to_csv("stack.csv")

    if(job_assigned == 0):
      print("No Job assigned")
      car_job_information_dict['Job_ID_assigned'] = 0
      car_job_information_dict['Job_total_completion_time'] = 0
    else:
      print("ID of the job assigned is",Job_id_number)
      print("Size of the job is", Job_size)
      print("Total Job Processing time", Job_total_completion_time)
      car_job_information_dict['Job_ID_assigned'] = job_assigned
      car_job_information_dict['Job_total_completion_time'] = Job_total_completion_time



    # print(car_job_information_dict)
    car_job_information_df = car_job_information_df.append(car_job_information_dict, ignore_index=True)
    car_job_information_dict.clear()
    

csv_data = car_job_information_df.to_csv("Car_and_Job_information_strategy_2.csv")

car_job_information_df.plot(x='Residency_time_seconds', y='Job_total_completion_time', style='o')

"""### Strategy 3
Strategy 3 is same as strategy 2 but you are dividing bigger jobs into 2 jobs and pushing it on stack to make sure job completion time is less than incoming vehicle residency time. When ever vehicle comes calculate residency time of the vechicle and check whether is there any jobs that has total job completion time less than residency time of vehicle and if yes then assign that job on that stack to that vehicle If no such job is found then assign new job to that vehicle if total job completion time is lesser than residency time of the vehicle
"""

import csv
with open('stack_strategy_3.csv', 'w') as creating_new_csv_file: 
  writer = csv.writer(creating_new_csv_file)
  writer.writerow(['Job_ID_assigned', 'Job_total_completion_time','status']) 
# print("Empty File Created Successfully")

import pandas as pd
job_stack_df = pd.DataFrame(columns=['Job_ID_assigned', 'Job_total_completion_time','status'])
car_job_information_df = pd.DataFrame(columns=['Vehicle_ID', 'Vehicle_speed','Arrival time','Residency_time_seconds', 'Job_ID_assigned', 'Job_total_completion_time'])

job_stack_df.drop(job_stack_df.index, inplace=True)
car_job_information_df.drop(car_job_information_df.index, inplace=True)



#Maximum highway speed 60 kilometers per hour or 60000 meters per hour
#Highway patrol police make sure that cars entered are not jammed in traffic on highway

import random
import numpy as np
import string
import time



# defining function for random
# string id with parameter
def ran_gen(size, chars=string.ascii_uppercase + string.digits):
    return ''.join(random.choice(chars) for x in range(size))

car_job_information_dict = {}

for i in range(500): 
  # function call for random string
  # generation with size 8 and string
  Vehicle_number = ran_gen(8, "AEIOSUMA23")
  car_job_information_dict['Vehicle_ID'] = Vehicle_number

  np.random.seed(random.randint(0,100000))
  Vehicle_arrival_time = random.randint(0,3600) # in seconds
  car_job_information_dict['Arrival time'] = Vehicle_arrival_time


  import time

  # np.random.seed(int(time.time()))
  np.random.seed(random.randint(0,100000))
  In_speed_kilometers_per_hour = abs(np.random.normal(size = 4, loc = 40, scale = 10)[0]) # geneating speed mean = 30 , standard_deviation = 20

  In_speed_meters_per_second = In_speed_kilometers_per_hour * 1000/3600
  car_job_information_dict['Vehicle_speed'] = In_speed_meters_per_second

  Residency_time_seconds = Highway_Total_distance/In_speed_meters_per_second
  car_job_information_dict['Residency_time_seconds'] = Residency_time_seconds

  found = 0
  with open('stack.csv') as file_obj:
      
    # Create reader object by passing the file 
    # object to reader method
    reader_obj = csv.reader(file_obj)
      
    # Iterate over each row in the csv 
    # file using reader object
    for row in reader_obj:
        # print(row)
        if(row[2]=="not_assigned" ):
          # print(row[1])
          if(Residency_time_seconds > float(row[1]) ):
            found = 1
            car_job_information_dict['Job_ID_assigned'] = row[0]
            car_job_information_dict['Job_total_completion_time'] = row[1]
  
  if(found == 0):
    Job_id_number = ran_gen(3, "AEIOSUMA23")
    Job_size = 10 #mb

    np.random.seed(random.randint(0,100000))
    Job_Processing = abs(np.random.normal(size = 4, loc = 700, scale = 40)[0])  # geneating job processing time mean = 400 , standard_deviation = 40

    Job_download_time = 1 # seconds
    Job_upload_time = 1 # seconds
    Job_total_completion_time = Job_download_time + Job_Processing + Job_upload_time

    if(Residency_time_seconds > Job_total_completion_time):
      job_assigned = Job_id_number
    elif (Residency_time_seconds < Job_total_completion_time):
      job_assigned = Job_id_number
      Job_Processing_first_half = Job_total_completion_time - Residency_time_seconds - 5
      Job_total_completion_time_first_half = Job_download_time + Job_Processing_first_half + Job_upload_time
      Job_total_completion_time = Job_Processing_first_half
      Job_total_completion_time = Job_total_completion_time_first_half

      Job_id_number_second_half = ran_gen(3, "AEIOSUMA23")
      Job_size = 10 #mb
      Job_Processing_second_half = Job_total_completion_time - Job_total_completion_time_first_half
      fields2 = []
      fields2.append(Job_id_number_second_half)
      fields2.append(Job_Processing_second_half)
      fields2.append("not_assigned") 
      with open(r'stack_strategy_3.csv', 'a') as f:
        writer = csv.writer(f)
        writer.writerow(fields2)
        fields2.clear()
    else:
      job_assigned = 0
      fields = []
      fields.append(job_assigned)
      fields.append(Job_total_completion_time)
      fields.append("not_assigned") 
      with open(r'stack_strategy_3.csv', 'a') as f:
        writer = csv.writer(f)
        writer.writerow(fields)
        fields.clear()
      # stack_df_refine = pd.read_csv('stack.csv')
      # df_new = stack_df_refine[stack_df_refine["Job_ID_assigned"].str.contains(job_assigned) == False]
      # df_new.to_csv("stack.csv")

    if(job_assigned == 0):
    # print("No Job assigned")
      car_job_information_dict['Job_ID_assigned'] = 0
      car_job_information_dict['Job_total_completion_time'] = 0
    else:
      print("ID of the job assigned is",Job_id_number)
      print("Size of the job is", Job_size)
      print("Total Job Processing time", Job_total_completion_time)
      car_job_information_dict['Job_ID_assigned'] = job_assigned
      car_job_information_dict['Job_total_completion_time'] = Job_total_completion_time



    # print(car_job_information_dict)
    car_job_information_df = car_job_information_df.append(car_job_information_dict, ignore_index=True)
    car_job_information_dict.clear()
    

csv_data = car_job_information_df.to_csv("Car_and_Job_information_strategy_2.csv")

car_job_information_df.plot(x='Residency_time_seconds', y='Job_total_completion_time', style='o')

"""You can see that for strategy 3 job completion time has decreased a lot compared to strategy 1 and strategy 2.

### Strategy 4 - Using AI
Using AI to choose which car to migrate. Suppose if many cars come at same time or we have to migrate VM to different cars. Again lets suppose all of them have same speed and same residency time. Then we can use AI to choose best car based on it  specifications because few cars also break down during highway due to poor enginer performance. Some cars also will meet with accident when break failure happens due to its poor maintanance or if it car was highly used by its previous owners. So when ever break down or accident or enginer failure or break failure happens not only car but also job assigned to it is also lost. So using AI to choose best car to migrate VM or while assigning jobs to the car is the best thing.

For this sake, I have collected dataset and built model which gives performance score of the car. 0 for poor performance car and as the score increases implies performance also increases. We have used several methods while training this dataset and choose the method for the model which gives best accuracy.

The dataset has following columns  </br>

"Name": Name of car for example: Maruti Wagon R, Honda etc </br>
"Location": Location of the car </br>
"Year": Purchased year of the car </br>
"Kilometers": Kilometers of the car used </br>
"Fuel_Type": Type of the fuel of the car such as "petrol", "Diesel" etc </br>
"Transmission": Transmission type of the car whether it is manual or automatic
"Owner_Type": Owner type of the car whether it is first ownered car or second ownered car </br>
"Mileage": Mileage of the car </br>
"Engine": Engine power of the car for example 950 CC, 1000 CC etc </br>
"power":Power of the car for example 60 bhp, 100 bhp </br>
"Seats": Number of seats in a car </br>
"New_Price": New price of the car </br>

1. EDA
2. Data Cleaning & Data Correction
3. Feature Engineering & Visualization
4. Model Training and Model Evaluation
5. Cross Validation
6. Conclusion
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

"""Load Dataset"""

df = pd.read_csv("/content/train-data.csv")

df

df.shape

"""Drop Unwanted Columns"""

df.drop(["Unnamed: 0","New_Price"],axis=1,inplace=True)

df

"""Check Null values

"""

df.isnull().sum()

plt.figure(figsize=(7,5))
sns.heatmap(df.isnull())
plt.show()

"""Data Cleaning and Correction of all Features

"""

df['Location'].value_counts()

df['Fuel_Type'].value_counts()

df[(df['Fuel_Type']=='Electric') | (df['Fuel_Type']=='LPG')]

i =df[(df['Fuel_Type']=='LPG') | (df['Fuel_Type']=='Electric')].index
df.drop(i,axis=0,inplace=True)

"""conclusion:
There are very very less car with LPG and Electric as fuel system so we can drop it
"""

df['Transmission'].value_counts()

df['Owner_Type'].value_counts()

i = df[df['Owner_Type']=='Fourth & Above'].index
df.drop(i,axis=0,inplace=True)

df['Seats'].value_counts()

df[df['Seats']>8]

df['Seats']=np.where(df['Seats']>8,6,df['Seats'])
df.drop(df[df['Seats']==0].index,axis=0,inplace=True)
df['Seats'].value_counts()

"""conclusion:
10 setaers and 0 seats sedan cars are not availables in realife
so we can replace 10 setaers with mode of seats lets say 6
"""

df['Name'].value_counts()

df['Name'].nunique()

"""Features spliting (Car & Model)

"""

df['Model']=df['Name'].map(lambda x: str(x).strip().split(" ")[1])
df['Name']=df['Name'].map(lambda x: str(x).strip().split(" ")[0])
df

df['Model'].value_counts()

df['Model'].value_counts()[df['Model'].value_counts()<5]

"""conclusion:
As seen here there are total 1862 types of cars which is very high
we have make seperate features for car model and car manufactures ie(Name) by feature spliting
"""

df['Year'].value_counts()

"""Covert data types into object
Year should be in object as its is category not a Numerical Number
"""

df['Year']=df['Year'].astype(str)
df['Year']=np.where(df['Year'].astype(int)<2003,"less than 2003",df['Year'])
df['Year'].value_counts()

"""conclusion:
As seen here there are total 22 differents years of car model
Cars with Years 2000,2001,2002,1998 & 1999 are very very less so we can make sepearte categ of "less than 2003" for all
"""

df['Kilometers_Driven'].isnull().sum()

df.dtypes

df.isnull().sum()

df['Mileage'].unique

df['Mileage']=df['Mileage'].map(lambda x: str(x).strip().split(" ")[0])

"""Convert data type into float

"""

df['Mileage']=df['Mileage'].astype(float)

"""conclusion:
Data correction of Mileage feature (Numerical feature)
Then we convert data to float
"""

df['Engine'].unique

"""Data correction of Engine by paasing it through function fun()"""

def fun(x):
    if x == None:
        return x
    elif len(str(x).split(" "))==2:
        return str(x).split(" ")[0]
df['Engine']=df['Engine'].apply(fun)
df['Engine'].isnull().sum()

"""Convert data type into float"""

df['Engine']=df['Engine'].astype(float)

"""Replace Null values of Engine column by mean base on model"""

lst = df['Model'].unique()
for i in lst:
    mean_model = df[df["Model"]==i]['Engine'].mean()
    df.update(df[df['Model']==i]['Engine'].fillna(mean_model))

df['Power'].isnull().sum()

df['Power'].unique

"""Data correction of power by paasing it through function"""

df['Power']=df['Power'].apply(fun)
df['Power'].unique

df['Power'].isnull().sum()
df[df['Power'].isnull()]

"""Replace Null values of Power column by mean base on model"""

lst = df['Model'].unique()
for i in lst:
    mean_model = df[df["Model"]==i]['Seats'].mode(0)[0]
    df.update(df[df['Model']==i]['Seats'].fillna(mean_model))
df[df['Power'].isnull()]

"""Convert data type of power into float"""

df['Power'] = pd.to_numeric(df['Power'],errors='coerce')
a = df['Model'].unique()
for i in a:
    r = df[df["Model"]==i]['Power'].astype(float).mean()
    df.update(df[df['Model']==i]['Power'].fillna(r))
df.dropna(axis=0,inplace=True)

"""Check Null values again"""

df.isnull().sum()

"""Check data types"""

df.dtypes

"""Conclusion:
We have done EDA, data cleaning and data conversion of all features
Now we will visualise all features
we will sort out all important features which dominate more to predict output or target feature ie Performance_Score

Feature Engineering & Visualization

Total Cars of all Companies
"""

plt.figure(figsize=(12,10))
most_cat = df['Name'].value_counts()
sns.barplot(x=most_cat, y=most_cat.index, data=df)
plt.xticks(size=15)
plt.xlabel("Frequency",size=22,c="g")
plt.ylabel("Manufactures",size=22,c="g")
plt.title("Total Cars of all Companies",size=28,c="r")
plt.show()

"""conclusion :
As seen Bently, Force, Lamborghini, and Isuzu cars are very very less
so these will act as outlier we will drop it
"""

df['Name'].value_counts()[df['Name'].value_counts()<5]

i = df['Name'].value_counts()[df['Name'].value_counts()<5].index
index = df[df['Name'].isin(i)].index
df.drop(index,axis=0,inplace=True)

"""Total Cars Location wise"""

plt.figure(figsize=(8,6))
most_cat = df['Location'].value_counts()
sns.barplot(x=most_cat, y=most_cat.index, data=df)
plt.xticks(size=15)
plt.xlabel("Frequency",size=15,c="g")
plt.ylabel("Location",size=15,c="g")
plt.title("Total Cars Location wise",size=22,c="r")
plt.show()

"""conclusion :
As seen cars from mumbai hyderbad and Kochi are very high
cars from ahmedabad are very less as compared to all
so we can say that location is some what imp feature

Performance Scores of differents manufactures
"""

plt.figure(figsize=(12,8))
sns.boxplot(data=df,y="Name",x="Performance_Score")
plt.xlabel("Performance_Score",size=18,c="g")
plt.ylabel("Manufactures",size=18,c="g")
plt.title("Performance Scores range of differents manufactures",size=25,c="r")
plt.xticks(rotation='vertical',size=15)
plt.show()

"""conclusion :
Here clearly we can see that performance of cars more depends on its brand
range of performance is different for differents brand

All Categ with their Rating
"""

plt.figure(figsize=(12,8)) 
sns.scatterplot(data=df,y="Name",x='Performance_Score',color="m")
plt.xticks(rotation='vertical',size=15)
plt.xlabel("Rating",size=15,c="r")
plt.ylabel("Category",size=15,c="r")
plt.title("All Categ with their Rating",size=25,c="red")
plt.show()

plt.figure(figsize=(8,6))
sns.boxplot(data=df,x="Fuel_Type",y="Performance_Score")
plt.xlabel("Fuel Types",size=17,c="k")
plt.ylabel("Performance_Score",size=17,c="k")
plt.title("Fuel Types",size=25,c="red")
plt.show()

"""conclusion :
See Performance Scores for different fuel types
Performance Scores of diesel cars are more than petrol
"""

plt.figure(figsize=(15,8))
sns.boxplot(data=df,x="Year",y="Performance_Score")
plt.xlabel("Year",size=20,c="g")
plt.ylabel("Performance_Score",size=20,c="g")
plt.title("Year vs Performance_Score",size=25,c="r")
plt.show()

"""conclusion :
so clearly seen that as car become older its Performance Score get reduces
Year of car is imp to predict output
"""

plt.figure(figsize=(8,5))
sns.countplot(data=df,x="Seats")
plt.xlabel("Seats",size=18,c="g")
plt.ylabel("Frequency",size=18,c="g")
plt.show()

"""conclusion :
Frequency of 5 seaters cas is very very high as compared to other
But ites not so imp to predict output
"""

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
sns.countplot(data=df,x="Owner_Type")
plt.xlabel("Owner Type",size=15,c="r")
plt.ylabel("Frequency",size=15,c="r")
plt.subplot(1,2,2)
labels =df['Owner_Type'].value_counts(sort = True).index
sizes = df['Owner_Type'].value_counts(sort = True)
plt.pie(sizes, labels=labels,autopct='%1.1f%%', shadow=True, startangle=270)
plt.title('Total % of Owner',size = 25,c="r")
plt.show()

"""conclusion :
percentages of 1st Owners is very high ie 82 %
"""

df.describe()

"""Handle Outliers"""

col = ['Kilometers_Driven','Power','Mileage','Engine'] 
plt.figure(figsize=(18,12))
for i,v in enumerate(col):
    print(i,v)
    plt.subplot(3,2,i+1)
    sns.boxplot(x=v, data=df,color='red')
plt.show()

df.drop(df[df['Kilometers_Driven']>300000].index,inplace=True,axis=0)

df.drop(df[df['Power']>480].index,inplace=True,axis=0)

"""Visualize after removing Major outliers

"""

col = ['Kilometers_Driven','Power','Mileage','Engine'] 
plt.figure(figsize=(18,12))
for i,v in enumerate(col):
    print(i,v)
    plt.subplot(3,2,i+1)
    sns.boxplot(x=v, data=df,color='green')
plt.show()

df.shape

"""Data distribution of output Performance Score feature

"""

plt.figure(figsize=(11,7))
plt.subplot(1,1,1)
sns.distplot(df['Performance_Score'],color='r',kde_kws={'linewidth':3,'color':'b'});
plt.show()

col = ['Kilometers_Driven','Mileage','Engine',"Performance_Score"] 
plt.figure(figsize=(18,12))
for i,v in enumerate(col):
    print(i,v)
    plt.subplot(3,2,i+1)
    sns.distplot(x=df[v],color='g')
plt.show()

plt.figure(figsize=(12,6))
sns.barplot(data=df,y="Performance_Score",x='Location',hue='Fuel_Type')
plt.xlabel("Location",size=15,c="r")
plt.ylabel("Performance_Score",size=15,c="r")
plt.title("Effect of fuel type on Car Performance_Score for location",size=25,c="red")
plt.show()

"""conclusion :
Here clearly we can see that Performance Score of Diesel car is more than than petrol irespective of location
"""

plt.figure(figsize=(16,6))
sns.barplot(data=df,y="Performance_Score",x='Name',hue='Fuel_Type')
plt.xlabel("Location",size=15,c="r")
plt.ylabel("Performance_Score",size=15,c="r")
plt.xticks(rotation="vertical",size=10)
plt.title("Effect of fuel type on Performance_Score of differents brands",size=25,c="red")
plt.show()

"""conclusion :
Here clearly we can see that Performance_Score of Diesel car for some brands such as jaguar and BMW is less than petrol
"""

col = ['Kilometers_Driven','Mileage','Engine',"Power"] 
plt.figure(figsize=(18,12))
for i,v in enumerate(col):
    print(i,v)
    plt.subplot(3,2,i+1)
    sns.scatterplot(x=df[v],y=df['Performance_Score'],color='purple')
plt.show()

plt.figure(figsize=(12,6))
sns.barplot(data=df,y="Performance_Score",x='Location',hue='Owner_Type')
plt.xlabel("Location",size=15,c="red")
plt.ylabel("Performance_Score",size=15,c="red")
plt.show()

plt.figure(figsize=(16,6))
sns.barplot(data=df,y="Performance_Score",x='Name',hue='Transmission')
plt.xlabel("Location",size=15,c="red")
plt.ylabel("Performance_Score",size=15,c="red")
plt.xticks(rotation="vertical",size=10)
plt.show()

"""conclusion :
Car with Automatic Transmission system has more Performance_Score than manual that is OK
But In real life practise Performance_Score of car is not highly depends on Transmission system
"""

plt.figure(figsize=(10,7))
sns.heatmap(df[["Performance_Score","Kilometers_Driven","Power","Mileage","Engine","Seats"]].corr(), annot=True,linewidths=.5,fmt='.1f')
plt.title("Correlation Graph",c="r",size=25)
plt.show()

"""conclusion :
Here we can see that Engine Power has strong positive realtion with Performance_Score
But we can also realise that Engine and Power itself has strong relation So In order to avoid multicolinearity problem we will drop one which has less correlation with o/p. 
Power has good relation with Performance_Score as compared to Engine so we drop Engine
Clearly we can say that more Engine CC more Power more the Performance_Score of car but one power feature is sufficent to get all info
"""

df.drop(['Mileage','Seats','Engine','Transmission'],axis=1,inplace=True)

"""Features Construction : 
We create single feature by combining Compnay Name and Its model
so we will have completely new feature of name Car which store info of Car and its model. 
Performance_Score of car is base not only on its company, its model also matter
If you are buying BMW 2 series and 7 series, company is same but in Performance_Score there is lots of difference.
"""

df['Car'] = df['Name'].astype(str) + " " + df['Model']

"""Drop unwanted columns"""

df.drop(['Name','Model'],axis=1,inplace=True)

df = df[['Car','Location','Year','Kilometers_Driven','Owner_Type','Fuel_Type','Power','Performance_Score']]

df['Car'].unique

"""See total no of categories of cars"""

len(df['Car'].value_counts())

lst = df['Car'].unique()
no_models= []
for i in lst:
    df1=df[df['Car']==i]
    no_models.append(len(df1))

dic = dict(zip(list(lst),no_models))
sort_orders = sorted(dic.items(), key=lambda x: x[1])
for i in sort_orders:
    print(i[0], i[1])

sortedDict = dict( sorted(dic.items(), key=lambda x: x[0].lower()) )
for k,v in sortedDict.items():
    print('{}:{}'.format(k,v))

"""Features Selection: As seee there are 200 cars categories
so we will drop all categories with very less frequency or counts
These will really helpful as its reduce computation time and improve machine effectiveness
"""

len(df['Car'].value_counts()[df['Car'].value_counts()<10])

df['Car'].value_counts()[df['Car'].value_counts()<10]

"""Lets drop categ with frequecy less than 10"""

l = df['Car'].value_counts()[df['Car'].value_counts()<10].index
df[df['Car'].isin(l)]

df.drop(df[df['Car'].isin(l)].index,inplace=True,axis=0)
df_new = df.copy()

"""shape of Data after EDA"""

df.shape

df.dtypes

df[['Car','Location','Year','Owner_Type','Fuel_Type']].nunique()

"""conclusion: We have consider All important features base on above visualisation and domian knowledge"""

df1=df.copy()

"""Encoding of Categorical features"""

from sklearn.preprocessing import LabelEncoder

"""Label Encoding on 'Car','Location' and 'Fuel_Type' columns"""

le = LabelEncoder()
lst = ['Car','Location','Fuel_Type']
for i in lst:
    le = LabelEncoder()
    df[i]=le.fit_transform(df[i])

"""Ordinal Encoding on 'Owner_Type' and 'Year'"""

df['Owner_Type'].value_counts()

dic = {'First':2,'Second':1,'Third':0}
df['Year'].value_counts()

dic1 = {'2019':17,"2018":16,"2017":15,"2016":14,"2015":13,"2014":12,"2013":11,"2012":10,"2011":9,"2010":8,"2009":7,"2008":6,"2007":5,"2006":4,"2005":3,"2004":2,"2003":1,"less than 2003":0 }
df['Owner_Type']=df['Owner_Type'].map(dic)
df['Year']=df['Year'].map(dic1)

"""Model Training and Model Evaluation"""

X = df.drop("Performance_Score",axis=1)   #X --- > Independent variables
Y= df['Performance_Score']                #Y --- > Dependent variables
Y

"""Train-Test split (80/30 ratio)

"""

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=1)

"""LinearRegression"""

from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(X_train, Y_train)
y_pred= lr.predict(X_test)
print("Accuracy on Traing set: ",lr.score(X_train,Y_train))
print("Accuracy on Testing set: ",lr.score(X_test,Y_test))

"""RandomForestRegressor"""

X_test.head()

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_estimators=100)
rf.fit(X_train, Y_train)
y_pred= rf.predict(X_test)
print(type(X_test))
print("Accuracy on Traing set: ",rf.score(X_train,Y_train))
print("Accuracy on Testing set: ",rf.score(X_test,Y_test))

"""Use Predict function to get performance score. The higher the score implies better the performance """

details1 = { 'Car': ['93'],	'Location': ['3'],	'Year' :['15'],	'Kilometers_Driven':['50000'],	'Owner_Type':	['2'], 'Fuel_Type':['2'],	'Power':['37'] }
details2 = { 'Car': ['93'],	'Location': ['3'],	'Year' :['22'],	'Kilometers_Driven':['2000'],	'Owner_Type':	['2'], 'Fuel_Type':['2'],	'Power':['37'] }

# creating a Dataframe object 
df = pd.DataFrame(details1)
y_pred1= rf.predict(df)
print(y_pred1)

df = pd.DataFrame(details2)
y_pred2= rf.predict(df)
print(y_pred2)

"""We found that RandomForestRegressor is better performing that LinearRegression. So we use RandomForestRegressor for our model

Model Evaluation
"""

from sklearn import metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error

print("\t\tError Table")
print('Mean Absolute Error      : ', metrics.mean_absolute_error(Y_test, y_pred))
print('Mean Squared  Error      : ', metrics.mean_squared_error(Y_test, y_pred))
print('Root Mean Squared  Error : ', np.sqrt(metrics.mean_squared_error(Y_test, y_pred)))
print('R Squared Error          : ', metrics.r2_score(Y_test, y_pred))

"""On Conclusion, We can say that we can use this model built to choose best performing car to migrate VM or while assigning jobs to the car. So that when ever break down or accident or engine failure or break failure happens we can avoid not only car damage but also we can avoid losing of the job assigned to it."""

